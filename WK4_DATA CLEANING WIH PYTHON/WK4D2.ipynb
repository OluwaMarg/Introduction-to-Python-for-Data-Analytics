{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA CLEANING USING PYTHON: HANDLING OUTLIERS\n",
        "##Definition and Causes of Outliers\n",
        "\n",
        "An outlier is a data point that is distant from other observations in a dataset. More formally, outliers are observations that deviate markedly from other members of the sample in which they occur.\n",
        "\n",
        "There are several potential causes of outliers:\n",
        "\n",
        "* **Measurement Error:** Outliers can occur due to mistakes in recording or collecting data, such as typos, incorrect sensor calibrations, or data entry errors.\n",
        "\n",
        "* **Execution Error:** Errors in execution can lead to outliers, such as bugs in data transmission or accidental duplication of records.\n",
        "\n",
        "* **Natural Variability:** Inherent variability and heterogeneity in the data may produce outliers.\n",
        "\n",
        "* **Intentional Outlier:** An outlier may be injected into the data intentionally as part of data poisoning attacks.\n",
        "\n",
        "* **Change in Behavior:** Sudden shifts or unusual occurrences in the system or phenomenon can result in outlier data points.\n"
      ],
      "metadata": {
        "id": "DlxpnOIzAGlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Dangers of Outliers\n",
        "Outliers can negatively impact the performance and reliability of machine learning models if not handled properly. Some key dangers of outliers include:\n",
        "\n",
        "* **Bias:** Outliers can skew and dominate model fitting, increasing bias in the model.\n",
        "\n",
        "* **Overfitting:** Models may overfit outliers and fail to generalize well on new data.\n",
        "\n",
        "* **Misinterpretation:** Anomalous data points can lead to false insights and misleading conclusions about the relationships in the data.\n",
        "\n",
        "* **Loss of Power:** Outliers reduce the power and reliability of statistical tests and modeling procedures.\n",
        "\n",
        "* **Incorrect Predictions:** Models may make erroneous predictions on new data if outliers in training data are not appropriately handled."
      ],
      "metadata": {
        "id": "40Ixv5jaB3ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecting Outliers in Univariate Data\n",
        "For univariate data where we only have a single variable, there are several simple methods to identify potential outliers:\n",
        "\n",
        "### a. Visualization for Univariate Outliers\n",
        "\n",
        "Visual inspection of the data using histograms, boxplots, and scatter plots can reveal outliers. See example below:\n",
        "\n",
        "(N/B The visual plots clearly reveal the presence of an outlier value.)"
      ],
      "metadata": {
        "id": "0DgiWJeBCX3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate sample data\n",
        "x = np.random.normal(size=1000)\n",
        "x[20] = 30 # Inject outlier\n",
        "\n",
        "# Histogram\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(x, color='blue', edgecolor='black', alpha=0.7)\n",
        "plt.title('Histogram of Data with Outlier')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvspan(30 - 1.5, 30 + 1.5, color='red', alpha=0.2)\n",
        "plt.axvline(30, color='red', linestyle='--', label='Outlier')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Boxplot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.boxplot(x, vert=False, patch_artist=True)\n",
        "ax.set_ylim(-12, 32)\n",
        "ax.set_title('Boxplot of Data with Outlier')\n",
        "ax.set_xlabel('Value')\n",
        "plt.yticks([])\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(range(len(x)), x, color='blue', label='Data')\n",
        "plt.scatter(20, 30, color='red', label='Outlier')\n",
        "plt.title('Scatter Plot of Data with Outlier')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pprNw9jRCjP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Standard Deviation Method\n",
        "Any points that are a certain number of standard deviations away from the mean may be flagged as outliers. Commonly used thresholds are 2, 2.5 or 3 standard deviations.\n",
        "\n",
        "The sample code below will print any values exceeding the chosen threshold, which is typically set based on domain knowledge and exploratory analysis. A lower threshold will flag more potential outliers."
      ],
      "metadata": {
        "id": "oOPEsWKlCpaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample dataset (replace with your data)\n",
        "x = np.array([1, 2, 3, 4, 5, 6, 1000])\n",
        "\n",
        "# Set threshold\n",
        "threshold = 3\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "mean = np.mean(x)\n",
        "std = np.std(x)\n",
        "\n",
        "# Determine outliers\n",
        "outlier_idx = np.abs(x - mean) > threshold * std\n",
        "print(x[outlier_idx])"
      ],
      "metadata": {
        "id": "mSu-YQH0DHdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. Interquartile Range Method\n",
        "Another approach is to use the interquartile range (IQR) to identify outliers. The IQR represents the middle 50% of the data and is less influenced by outliers. Any points below Q1 - 1.5IQR or above Q3 + 1.5IQR are marked as outliers, where Q1 and Q3 are the first and third quartiles respectively. The 1.5 multiplier helps set a stringent boundary for outliers while accounting for regular variability around the upper and lower quartiles. This relies on the IQR rather than standard deviation to set the outlier bounds."
      ],
      "metadata": {
        "id": "sdQkn44hDVJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define your dataset 'x' (replace this with your data)\n",
        "x = [10, 15, 20, 22, 25, 30, 35, 100, 105, 110]\n",
        "\n",
        "# Calculate Q1, Q3, and IQR\n",
        "Q1 = np.percentile(x, 25)\n",
        "Q3 = np.percentile(x, 75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Determine outlier bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Find outlier indices\n",
        "outlier_indices = np.where((x < lower_bound) | (x > upper_bound))\n",
        "outliers = np.array(x)[outlier_indices]  # Convert x to a NumPy array and then use the indices\n",
        "print(\"Outliers:\", outliers)"
      ],
      "metadata": {
        "id": "7CaRmU8zDa15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecting Outliers in Multivariate Data\n",
        "Multivariate data analysis involves considering the relationships between different features, making the identification of outliers a more intricate task. In this guide, we will explore various methods for detecting multivariate outliers, and we will use Python in a Jupyter notebook to demonstrate these techniques.\n",
        "\n",
        "### a. Visual Methods\n",
        "Visual methods provide a powerful way to spot multivariate outliers through plots. Two commonly used visual methods are Scatterplot Matrices and Parallel Coordinate Plots. We will generate sample data and use these plots to identify outliers.\n",
        "\n",
        "Points outside the general scope of the data will stand out as outliers in these plots."
      ],
      "metadata": {
        "id": "CuD2MkJ6EWOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoAdg8QN-Wuu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.plotting import parallel_coordinates\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(0)\n",
        "num_samples = 100\n",
        "num_features = 4\n",
        "\n",
        "data = {\n",
        "    'Feature1': np.random.normal(0, 1, num_samples),\n",
        "    'Feature2': np.random.normal(0, 1, num_samples),\n",
        "    'Feature3': np.random normal(0, 1, num_samples),\n",
        "    'Feature4': np.random.normal(0, 1, num_samples),\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Scatterplot matrix\n",
        "pd.plotting.scatter_matrix(df)\n",
        "plt.show()\n",
        "\n",
        "# Parallel coordinates plot\n",
        "df['NameOfClassifyingColumn'] = np.random.choice(['A', 'B', 'C'], num_samples)\n",
        "parallel_coordinates(df, 'NameOfClassifyingColumn')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Statistical Models\n",
        "Statistical models, like regression or multivariate density estimation, can be used to identify outliers by calculating the residuals and setting a threshold. Let’s apply Linear Regression for outlier detection."
      ],
      "metadata": {
        "id": "zWJjF5sEE0yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate sample data\n",
        "X = df[['Feature1', 'Feature2', 'Feature3', 'Feature4']]\n",
        "y = np.random.normal(0, 1, num_samples)\n",
        "\n",
        "reg = LinearRegression().fit(X, y)\n",
        "y_pred = reg.predict(X)\n",
        "\n",
        "residuals = y - y_pred\n",
        "threshold = 3 * residuals.std()\n",
        "outlier_idx = np.abs(residuals) > threshold"
      ],
      "metadata": {
        "id": "Iicp4lRHE4et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. Proximity-Based Methods\n",
        "Proximity-based methods involve using metrics like k-nearest neighbors and isolation forests to identify anomalies. These methods are based on the idea that outliers are isolated or distant from their neighbors.\n",
        "\n",
        "Let's use Local Outlier Factor and Isolation Forest for outlier detection.\n",
        "\n",
        "In the code below, we'll demonstrate the steps and techniques for detecting multivariate outliers using Python. You can apply these methods to your own datasets, whether they come from .csv files or are generated within the code."
      ],
      "metadata": {
        "id": "JhZwOhxqEjOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Local Outlier Factor\n",
        "lof = LocalOutlierFactor(n_neighbors=10)\n",
        "outlier_scores = lof.fit_predict(X)\n",
        "outlier_idx = outlier_scores == -1\n",
        "\n",
        "# Isolation Forest\n",
        "iso = IsolationForest(n_estimators=100)\n",
        "outlier_scores = iso.fit_predict(X)\n",
        "outlier_idx = outlier_scores == -1"
      ],
      "metadata": {
        "id": "urVJh2eGFI0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3Complete code for multivraite data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.plotting import parallel_coordinates\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(0)\n",
        "num_samples = 100\n",
        "num_features = 4\n",
        "\n",
        "# Create a DataFrame with random data\n",
        "data = {\n",
        "    'Feature1': np.random.normal(0, 1, num_samples),\n",
        "    'Feature2': np.random.normal(0, 1, num_samples),\n",
        "    'Feature3': np.random.normal(0, 1, num_samples),\n",
        "    'Feature4': np.random.normal(0, 1, num_samples),\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Scatterplot matrix\n",
        "pd.plotting.scatter_matrix(df)\n",
        "plt.show()\n",
        "\n",
        "# Parallel coordinates plot\n",
        "df['NameOfClassifyingColumn'] = np.random.choice(['A', 'B', 'C'], num_samples)\n",
        "parallel_coordinates(df, 'NameOfClassifyingColumn')\n",
        "plt.show()\n",
        "\n",
        "# Statistical Models - Linear Regression for Outlier Detection\n",
        "X = df[['Feature1', 'Feature2', 'Feature3', 'Feature4']]\n",
        "y = np.random.normal(0, 1, num_samples)\n",
        "\n",
        "reg = LinearRegression().fit(X, y)\n",
        "y_pred = reg.predict(X)\n",
        "\n",
        "residuals = y - y_pred\n",
        "threshold = 3 * residuals.std()\n",
        "outlier_idx = np.abs(residuals) > threshold\n",
        "\n",
        "# Proximity-Based Methods - Local Outlier Factor\n",
        "lof = LocalOutlierFactor(n_neighbors=10)\n",
        "outlier_scores = lof.fit_predict(X)\n",
        "outlier_idx = outlier_scores == -1\n",
        "\n",
        "# Proximity-Based Methods - Isolation Forest\n",
        "iso = IsolationForest(n_estimators=100)\n",
        "outlier_scores = iso.fit_predict(X)\n",
        "outlier_idx = outlier_scores == -1"
      ],
      "metadata": {
        "id": "VIAPNpKaFnk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above generates random sample data for multivariate outlier detection, creates scatterplot matrices, parallel coordinates plots, and applies linear regression, local outlier factor, and isolation forest for outlier detection."
      ],
      "metadata": {
        "id": "XqmhtyDPFMKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Outliers in Univariate Case\n",
        "Once outliers have been identified, there are several techniques to handle them:\n",
        "\n",
        "### a. Delete Outliers\n",
        "The simplest approach is to completely remove outliers from the dataset. However, this could discard potentially useful data and should be done cautiously."
      ],
      "metadata": {
        "id": "zi0Bp03gGBjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_filtered = X[~outlier_idx]\n",
        "y_filtered = y[~outlier_idx]"
      ],
      "metadata": {
        "id": "MQEEB9BtGM8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Impute Missing Values\n",
        "It is important to first handle any missing values in the data prior to outlier detection and removal. Imputing missing values with outliers present may skew results. Simple imputation or robust methods can be used to fill missing values."
      ],
      "metadata": {
        "id": "fjJPRRKLGODC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Impute Outliers\n",
        "Instead of deleting, outliers can be replaced with substituted values like the mean, median, or values from a model prediction. One common approach is to use a regression model to predict the values of outliers based on the non-outliers.\n",
        "\n",
        "* **Imputing Outliers with a Regression Model**\n",
        "\n",
        "One approach to handling outliers is to impute them using a regression model. This method involves training a model on the data without outliers, predicting the outlier values, and replacing them with the model’s predictions. While this technique can be effective in reducing the impact of outliers on your analysis or models, it’s important to consider its pros and cons and when it’s most suitable.\n",
        "\n",
        "* **Pros:**\n",
        "\n",
        "**Retaining Data Points:**  Imputing outliers allows you to retain all data points in your dataset. This can be crucial when you want to preserve information and maintain a complete dataset for further analysis.\n",
        "\n",
        "**Reduces Outlier Influence:** By imputing outliers with predicted values from a regression model, you mitigate the extreme influence of outliers on your analysis or machine learning models. This results in more robust and reliable results.\n",
        "\n",
        "**Preserves Data Characteristics:** The imputed values are derived from the relationships within the data, which helps preserve the overall characteristics of the dataset.\n",
        "\n",
        "* **Cons:**\n",
        "\n",
        "**Model Sensitivity:** The success of this method heavily depends on the quality of the regression model. If the model used for imputation is not well-suited to the data, it may lead to inaccurate imputations.\n",
        "\n",
        "**Assumptions:** This approach assumes that the relationships between variables can be well-captured by a linear regression model. If the data has complex or nonlinear relationships, this method may not perform well.\n",
        "\n",
        "**Overfitting:** There is a risk of overfitting, particularly when the dataset contains a small number of data points or when the model is too complex. Overfit models may generate imputed values that do not generalize well to new data.\n",
        "\n",
        "**When to Use It:**\n",
        "\n",
        "Imputing outliers with a regression model is a good choice when you want to retain all data points, and you have domain knowledge or evidence suggesting that the imputed values will provide a reasonable approximation of the true values.\n",
        "\n",
        "It is worth noting that there are alternative methods for imputing outliers, such as using the median or other robust statistics. These methods may be preferred when the dataset has unique characteristics that make regression-based imputation less suitable. The choice of imputation method should be guided by a thorough understanding of the data and the problem context.\n",
        "\n",
        "Note that Imputing outliers with a regression model can be a useful strategy when you want to retain the data points while reducing the influence of extreme values on your analysis or models.\n",
        "\n",
        "Let's demonstrate how this can be done using Python's scikit-learn library"
      ],
      "metadata": {
        "id": "tJ1A812FGgtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data with outliers\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=5, random_state=42)\n",
        "# Introduce an outlier\n",
        "X[0] = 2.0\n",
        "y[0] = 300.0\n",
        "\n",
        "# Identify outliers based on some threshold (e.g., 2 standard deviations)\n",
        "threshold = 2 * np.std(y)\n",
        "outliers = np.abs(y) > threshold\n",
        "\n",
        "# Create a copy of the data\n",
        "X_clean = X.copy()\n",
        "y_clean = y.copy()\n",
        "\n",
        "# Impute outliers by training a simple linear regression model\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X[~outliers], y[~outliers])\n",
        "y_clean[outliers] = regressor.predict(X[outliers].reshape(-1, 1))\n",
        "\n",
        "# Now, y_clean contains the imputed values for outliers"
      ],
      "metadata": {
        "id": "WSBxKTooHbrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. Capping\n",
        "Capping replaces outliers with the maximum non-outlier values in the tails of the distribution. For example, outliers above the upper quartile can be capped to the maximum value below the upper whisker. This retains presence of outliers without distorting the distribution.\n",
        "\n",
        "Note that Capping preserves outliers while limiting their impact."
      ],
      "metadata": {
        "id": "3SGuLfJtH3Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set upper cap\n",
        "x[x > Q3 + 1.5*IQR] = max(x[x <= Q3 + 1.5*IQR])\n",
        "\n",
        "# Set lower cap\n",
        "x[x < Q1 - 1.5*IQR] = min(x[x >= Q1 - 1.5*IQR])"
      ],
      "metadata": {
        "id": "55JHsAkwH8xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d. Robust Methods\n",
        "Robust statistical methods like RANSAC or Theil-Sen estimators can fit models that are resilient to outliers in data. Robust models limit the influence of outliers when fitting and making predictions. See this [video](https://www.youtube.com/watch?v=QWa_NxnN7hI) for more details."
      ],
      "metadata": {
        "id": "BXNlX47dID0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RANSACRegressor\n",
        "from sklearn.linear_model import TheilSenRegressor\n",
        "\n",
        "# RANSAC robust regression\n",
        "ransac = RANSACRegressor()\n",
        "model = ransac.fit(X, y)\n",
        "\n",
        "# Theil-Sen robust regression\n",
        "theil = TheilSenRegressor()\n",
        "model = theil.fit(X, y)"
      ],
      "metadata": {
        "id": "G0zl-MjlJP0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### e. Weighting\n",
        "Downweight outliers by assigning lower sample weights when model training. This reduces their impact without discarding them entirely.\n",
        "\n",
        "In our xamplebelow, we assign a weight of 0.2 to outliers while keeping other points at 1. Weighting preserves outliers but reduces their significance during modeling."
      ],
      "metadata": {
        "id": "M6kUG1ieJSXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate outlier weight vector\n",
        "outlier_weights = [1 if not o else 0.2 for o in outlier_idx]\n",
        "\n",
        "# Train model with outlier weighting\n",
        "model = LinearRegression()\n",
        "model.fit(X, y, sample_weight=outlier_weights)"
      ],
      "metadata": {
        "id": "zWDeC0dmJiL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Outliers in Multivariate Data\n",
        "\n",
        "Handling outliers in multivariate data is essential to ensure the robustness and reliability of machine learning models. When dealing with multiple features, the impact of outliers can be more complex and significant.\n",
        "\n",
        "We will explore two approaches for handling outliers in multivariate datasets:\n",
        "\n",
        "* Isolation Forest\n",
        "* Local Outlier Factor (LOF)."
      ],
      "metadata": {
        "id": "DFxD9nUiJzQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Isolation Forest\n",
        "\n",
        "The Isolation Forest is a tree-based algorithm that excels in isolating anomalies in the data. It works by constructing isolation trees, which are binary trees where nodes represent feature splits, and the depth of a data point in the tree corresponds to its anomaly score. Data points that reach a smaller depth are considered more isolated and are more likely to be outliers.\n",
        "\n",
        "Here's how you can use the Isolation Forest to handle outliers in a multivariate dataset:\n",
        "\n",
        "In this example, we set the `contamination` parameter to 0.05, indicating that we expect approximately 5% of the data to be outliers. Adjust this parameter based on your domain knowledge and the characteristics of your dataset."
      ],
      "metadata": {
        "id": "s_i55hsAKGUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Create an Isolation Forest model\n",
        "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "\n",
        "# Fit the model on the data\n",
        "iso_forest.fit(X)\n",
        "\n",
        "# Predict outliers\n",
        "outlier_pred = iso_forest.predict(X)\n",
        "\n",
        "# Filter data to remove outliers\n",
        "X_filtered_iso_forest = X[outlier_pred != -1]"
      ],
      "metadata": {
        "id": "g7941vzqKaO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local Outlier Factor (LOF)\n",
        "The Local Outlier Factor (LOF) is a density-based method that identifies anomalies by comparing the local density of a data point with the density of its neighbors. Data points with significantly lower densities than their neighbors are considered outliers.\n",
        "\n",
        "Here's how you can use the LOF algorithm to handle outliers in a multivariate dataset:\n",
        "\n",
        "In this example, we set the `n_neighbors` parameter to 20, indicating that the algorithm considers the density of 20 nearest neighbors. Adjust this parameter based on your dataset and desired sensitivity to outliers."
      ],
      "metadata": {
        "id": "WUWD53nRKrdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "# Create a Local Outlier Factor model\n",
        "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
        "\n",
        "# Fit the model on the data\n",
        "lof.fit(X)\n",
        "\n",
        "# Predict outliers\n",
        "outlier_pred_lof = lof.fit_predict(X)\n",
        "\n",
        "# Filter data to remove outliers\n",
        "X_filtered_lof = X[outlier_pred_lof != -1]"
      ],
      "metadata": {
        "id": "mWCwX2PfK5zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing the Right Method\n",
        "The choice between Isolation Forest and LOF depends on the nature of your data and the specific problem you are addressing. Isolation Forest is suitable for cases where anomalies are expected to be more isolated and sparse, while LOF is appropriate for cases where anomalies have a more local impact on density.\n",
        "\n",
        "Remember that both methods provide a way to filter out outliers and create a cleaner dataset for modeling. You can evaluate the performance of your machine learning models on both the filtered and unfiltered datasets to assess the impact of outlier removal.\n",
        "\n",
        "Handling outliers in multivariate data is crucial for improving the reliability and accuracy of your machine learning models. Proximity-based methods like Isolation Forest and Local Outlier Factor offer effective techniques for identifying and removing outliers, ultimately leading to more robust data analysis and modeling results."
      ],
      "metadata": {
        "id": "XcG3CYOFK7Ar"
      }
    }
  ]
}